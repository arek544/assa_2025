{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preliminaries\n",
    "## 1.1. Course Reminders: Models, Regression, Classification\n",
    " A (machine learning) model is essentially a mapping (a mathematical function $f$) between inputs $x$ and outputs $y$, such that $ y \\approx f(x)$. For instance, in age estimation, the input data $x$ is the set of pixel values of a person's photo, and the output is the number $y$ corresponding to the age of that person.\n",
    "\n",
    "<center><a href=\"https://pyimagesearch.com/2020/04/13/opencv-age-detection-with-deep-learning/\">\n",
    "    <img src=\"https://b2633864.smushcdn.com/2633864/wp-content/uploads/2020/04/opencv_age_detection_examples.jpg?lossy=1&strip=1&webp=1\" width=\"550\"></a></center>\n",
    "    \n",
    "To perform such a task, we have to:\n",
    "\n",
    "- Design a model $f$, that is, we specify a parameterized family of functions (e.g., a neural network). The number of parameters may range from a few to billions.\n",
    "- Train $f$ using a dataset of known image/age pairs (a *training dataset*) in order to adjust the parameters of the model. We do that by considering an *optimization* problem: ideally, we want the predicted age $y_{\\text{pred}} = f(x)$ to be as close as possible to the true age $y$. Therefore, we define a *loss function* which measures the difference between $y$ and $y_{\\text{pred}}$, and we want to adjust the parameters of the model $f$ such that they minimize this loss function. This is achieved by using an optimization algorithm such as stochastic gradient descent, which is directly implemented in Pytorch.\n",
    "\n",
    "Once the model is designed and trained, we can use it for performing predictions on novel data.\n",
    "\n",
    "Broadly speaking, there are two types of tasks in supervised machine learning, depending on the nature of the output $y$:\n",
    "- if $y$ can take only a finite number of values (usually represented as integers), then it is a *classification* task. The most basic case is when there are only two possible classes (binary classification or *detection*). Examples are hate speech detection (1 = hate, 0 = no hate), recommender systems (1 = like, 0 = dislike), spam detection (1 = spam, 0 = not spam), etc.\n",
    "- if $y$ ranges in a continuous (and therefore infinite) set of values, then it is a *regression* task. Examples are age prediction as above ($y \\in [0, 100]$, or any arbitrary high value), estimating the price of a product, weather forecast, etc.\n",
    "\n",
    "In this lab, we will briefly experiment with the Autograd module of Pytorch (**Section 2**), and then use it for both regression (**Section 3**) and classification (**Section 4**) using neural networks. But first, let's set up our Python kernel (**Section 1.2**)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Setting-up the kernel\n",
    "Please select a python kernel where pytorch and matplotlib are installed to run this notebook. If you don't have such an environment available, here is how to easily create and select one in Visual Studio Code:\n",
    "- Click _Select Kernel_ on the upper-right corner of this window or _Select Another Kernel_\n",
    "- Click _Python Environments..._\n",
    "- Click _+Create Python Environment_\n",
    "- Click _venv Manages virtual environments created using 'venv'_\n",
    "- Select a Python version installed on your computer\n",
    "- Choose a name for your environment (eg: assa25)\n",
    "- Skip package installation\n",
    "- Open a Terminal via the menu: Terminal -> New Terminal. The newly created environment should be activated in this terminal. If not, run the activation script in your newly created venv folder via: `source <envname>/bin/activate`\n",
    "- Install pytorch via `pip install torch` and matplotlib via `pip install matplotlib`.\n",
    "- To let your Jupyter notebook communicate back-and-forth with your python environment using the Jupyter messaging protocol, you will also need to install _ipykernel_ in it: `pip install ipykernel`\n",
    "- That's it ! You should be able to run the cell below.\n",
    "\n",
    "_Note: VSCode creates the venv folder in your project folder by default, which is ok for this tutorial. For more advanced or professional workflows, it is considered better practice to place all the venv folders in a centralized location, and to choose the most appropriate one from there._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Automatic differentiation (Autograd)\n",
    "\n",
    "Autograd is the Pytorch engine that performs gradient tracking and computation. It really is the core of neural networks training, since optimization algorithms like stochastic gradient descent rely on gradient computation. Every tensor in Pytorch not only contains some values (the data itself), but also stores the gradients related to operations that have been computed using these tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# By default, when tensors are created their gradients is not tracked\n",
    "x = torch.ones(1, 10)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change it using the requires_grad_() method\n",
    "x.requires_grad_()\n",
    "print(x.requires_grad)\n",
    "\n",
    "# Alternatively, when creating a tensor, you can directly set 'requires_grad=True'\n",
    "x = torch.ones(1, 10, requires_grad=True)\n",
    "print(x.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can access the gradient stored in x with 'x.grad'.\n",
    "# However, since no computation has been performed yet, it should return 'None'\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you create a tensor y from x using any torch operation, it will have a gradient function (grad_fn) which is specific to this operation\n",
    "y = x + 50\n",
    "print(y.grad_fn)\n",
    "\n",
    "y = 3 * x\n",
    "print(y.grad_fn)\n",
    "\n",
    "y = x.mean()\n",
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's define an operation involving a 1D data tensor $\\mathbf{x} = [x_0, \\ldots, x_{9}]$ and a parameter $a$:\n",
    "\n",
    "$$ loss = a \\times \\sum_i x_i. $$\n",
    "\n",
    "We can mathematically compute the gradient of $loss$ with respect to $a$ and $\\mathbf{x}$:\n",
    "\n",
    "$$ \\nabla_{a} loss = \\sum_i x_i $$\n",
    "and\n",
    "$$  \\nabla_{x_i} loss = a \\quad (\\text{or in vector form:} \\quad \\nabla_{\\mathbf{x}} loss = [a, \\ldots, a]). $$\n",
    "\n",
    "Of course, on this example it would be easy to compute gradients by hand. But when working with deep neural networks with millions of parameters, this is impractical. The good news is that Pytorch does it automatically for us, thanks to the `backward()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's implement the above:\n",
    "a = torch.tensor([3], requires_grad=True, dtype=torch.float) # specify float as the data type, otherwise it can't compute gradients\n",
    "x = torch.ones(1, 10, requires_grad=True)\n",
    "\n",
    "loss = x.sum() * a\n",
    "print(loss.requires_grad)\n",
    "\n",
    "# Now, let's compute the gradient of the loss with respect to x and a\n",
    "loss.backward()\n",
    "\n",
    "# These gradients are stored in x.grad and a.grad\n",
    "print(x.grad)\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: When we create a Pytorch model using `torch.nn` modules (we'll see that later on), the corresponding parameters have `requires_grad=True` by default. It makes sense since these parameters are exactly what we want to optimize (therefore we need to compute their gradients). However, in general we do not need to compute the gradient with respect to the data (input $x$ and/or output $y$), since these are fixed. In the example above, we computed the gradient with respect to `x` purely for illustrative purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After a model is trained, we no longer need to compute its gradient. Therefore, when doing prediction,\n",
    "# we can disable gradient tracking to save memory and computational time\n",
    "\n",
    "with torch.no_grad():\n",
    "    loss = x.sum() * a\n",
    "    print(loss.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - define a data tensors x which simply contains the value 2\n",
    "# - define another data tensor y which simply contains the value 10\n",
    "# - define parameter tensors w and b with values 1 and 3, respectively (these need gradient tracking)\n",
    "# - compute the loss as: loss=(y - (w*x + b))^2\n",
    "# - compute the gradients of the loss\n",
    "# - print the value of the gradient of loss with respect to w and b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Regression\n",
    "## 3.1. Linear regression\n",
    "\n",
    "<center><a href=\"https://kpu.pressbooks.pub/learningstatistics/chapter/linear-regression/\">\n",
    "    <img src=\"https://kpu.pressbooks.pub/app/uploads/sites/66/2019/09/regression1a-1.png\" width=\"500\"></a></center>\n",
    "\n",
    "Let's now address linear regression. Assuming we have several points of data pairs $(x,y)\\in\\mathbb{R}^2$, the model is simply $f(x) = wx + b$, where $w$ is called *weight* (or slope) and $b$ is called the *bias*. The goal of linear regression is to estimate the optimal $w$ and $b$ from the available training data $x$ and $y$.\n",
    "\n",
    "**Note**: we don't really need Pytorch for solving that problem (indeed, a basic gradient descent algorithm can be implemented directly without too much hassle - if you have some math background and want to have fun, please feel free). Nonetheless, we consider it anyway because it is a good basic exercice, and once linear regression is solved, we can move to more sophisticated nonlinear regression and to (deep) neural networks.\n",
    "\n",
    "### 3.1.1. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a toy dataset\n",
    "x = torch.tensor([[3.3], [4.4], [5.5], [6.71], [6.93], [4.168], [9.779], [6.182], [7.59],\n",
    "                 [2.167], [7.042], [10.791], [5.313], [7.997], [3.1]], dtype=torch.float)\n",
    "y = torch.tensor([[1.7], [2.76], [2.09], [3.19], [1.694], [1.573], [3.366], [2.596], [2.53],\n",
    "                 [1.221], [2.827], [3.465], [1.65], [2.904], [1.3]], dtype=torch.float)\n",
    "print(x.shape, y.shape)\n",
    "\n",
    "# Plot the data (y as a function of x)\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'bo')\n",
    "plt.xlabel('x'), plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: the data tensors `x` and `y` we have defined here constitute our *dataset*. Each data sample $(x,y)$ is actually one element in `(x,y)`. Therefore, we have one dataset of length 15, where each element ($x$ or $y$) has dimension 1. In other words, our dataset only consists of 1 batch, and the batch size is 15.\n",
    "\n",
    "### 3.1.2. Linear Model\n",
    "\n",
    "Let us create our linear model. In pytorch, it is easily done with `nn.Linear` (check the [documentation](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html?highlight=torch%20nn%20linear#torch.nn.Linear)!). In particular, we need to specify the input and output sizes. In our case, these are equal to 1 since we work with scalars. There is also an option `bias` which can be `True` (default) or `False`. If `bias=False`, then $b=0$ and the model reduces to $f(x) = wx$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the linear model\n",
    "input_size = 1\n",
    "output_size = 1\n",
    "lin_reg_model = nn.Linear(input_size, output_size, bias=True)\n",
    "\n",
    "# 'lin_reg_model' is a module which contains the linear function and its parameters (weight and bias)\n",
    "# When we instanciate the model, the parameters are initialized with random values\n",
    "# We also remark that both parameters have 'requires_grad=True', which is the normal default behavior.\n",
    "print(lin_reg_model.weight)\n",
    "print(lin_reg_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can compute predictions as y_pred = f(x) simply as follows:\n",
    "y_pred = lin_reg_model(x)\n",
    "\n",
    "# Since y_pred is computed from the model (which has requires_grad=True), then it also has requires_grad=True\n",
    "print(y_pred.requires_grad)\n",
    "\n",
    "# If we want to plot y_pred, we first need to get rid of its gradient (because by default it's tracked).\n",
    "# This is done as follows:\n",
    "y_pred = y_pred.detach()\n",
    "print(y_pred.requires_grad)\n",
    "\n",
    "# We can plot the predicted values on top of the original data\n",
    "plt.figure()\n",
    "plt.plot(x, y, 'bo', label='Original data')\n",
    "plt.plot(x, y_pred, 'r', label='Predicted data')\n",
    "plt.xlabel('x'), plt.ylabel('y')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to manually change the values of the parameters (weight and bias):\n",
    "lin_reg_model.weight.data.fill_(2)\n",
    "lin_reg_model.bias.data.fill_(3)\n",
    "\n",
    "print(lin_reg_model.weight)\n",
    "print(lin_reg_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - set the weight and bias of the linear model at 0.5 and -1, respectively.\n",
    "# - compute the new predictions y_pred with this model\n",
    "# - plot the predictions on top of the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. Training\n",
    "\n",
    "Now that our model is defined, we have to train it, that is, to optimize the values of $a$ and $b$. To do that, we first define the [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions) and the optimizer (i.e., [the optimization algorithm](https://pytorch.org/docs/stable/optim.html#algorithms))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: first, re-instanciate the model and set both the weight and bias at 1.\n",
    "# This will avoid random values and ensure reproducibility (everyone should get the same results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For linear regression, it is common to use the mean squared error (MSE) as loss function\n",
    "loss_fn = nn.MSELoss() \n",
    "\n",
    "# For the optimizer, let's use stochastic gradient (SGD). In Pytorch, we need to specify:\n",
    "# - which parameters are going to be updated (in this case, it's the parameters of our model)\n",
    "# - the learning rate\n",
    "optimizer = torch.optim.SGD(lin_reg_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate what happens when the loss function is noted $\\mathcal{L}$ and the optimization algorithm is (stochastic) gradient descent (with a parameter $\\mu$ called *learning rate*). The process of *training* is an iterative procedure, where at each iteration (or *epoch*), we perform the following operations:\n",
    "1. Using the current model's parameters, compute the predictions: $y_{\\text{pred}} = f(x)$\n",
    "2. Using the loss function, compute the error between true values and predictions: $l = \\mathcal{L}(y_{\\text{pred}},y)$\n",
    "3. Compute the gradients of the loss with respect to the parameters. If there are only two parameters $w$ and $b$, then we have to compute $\\nabla_{w} l$ and $\\nabla_{b} l$.\n",
    "4. Perform gradient descent, that is, update the parameters with: $w \\leftarrow w - \\mu \\nabla_{w} l$ and $b \\leftarrow b - \\mu \\nabla_{b} l$\n",
    "\n",
    "The good news is that we don't have to explicitly code steps 3. and 4., since Pytorch takes care about that for us with simple functions: all gradients are computed with `l.backward()` (we have seen this one already) and the gradient descent is simply performed with `optimizer.step()`.\n",
    "\n",
    "**Note**: before computing the gradients with `l.backward()`, it's safer to remove all possible gradients that have been computed beforehand (e.g., from another model, or loaded in the memory) to avoid issues. This is easily done with: `optimizer.zero_grad()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write the training loop for 10 epochs.\n",
    "# At the end of each epoch, print the value of the loss (it is easily accessed with 'l.item()')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training, save the parameters of the trained model and display them\n",
    "torch.save(lin_reg_model.state_dict(), 'model_linear_regression.pt')\n",
    "print(lin_reg_model.weight)\n",
    "print(lin_reg_model.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the final estimates y_pred with the trained model, and display the results (predictions and original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Nonlinear regression\n",
    "\n",
    "Let us now consider a more elaborate nonlinear regression model, for which we design our first neural network. In nonlinear regression, $f$ is no longer a simple linear/affine function, but something more complicated, or something that we don't really know.\n",
    "\n",
    "<center><a href=\"https://www.r-bloggers.com/2016/02/first-steps-with-non-linear-regression-in-r/\">\n",
    "    <img src=\"https://i0.wp.com/datascienceplus.com/wp-content/uploads/2016/02/NLS_2.png\" width=\"400\"></a></center>\n",
    "\n",
    "### 3.2.1. Nonlinear Data\n",
    "\n",
    "We now consider sinusoidal-like data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: generate sinusoidal data:\n",
    "# - create a tensor x which ranges from -3 to 3 with a step of 0.1\n",
    "# - Use unsqueeze(1) on x to add an extra dimension (its shape should be [60, 1])\n",
    "# - compute y = cos(x)\n",
    "# - plot y as a function of x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. Model : the simple perceptron\n",
    "\n",
    "A basic idea behind neural networks is to assemble simple layers to approximate a potentially complicated function. For instance, in multilayer perceptrons (MLP), a network is designed by concatenating several layers, where each layer consists of:\n",
    "- a linear function\n",
    "- a non-linear element-wise activation function\n",
    "\n",
    "There is a plethora of nonlinear activation function, which you can check [here](https://pytorch.org/docs/stable/nn.html#non-linear-activations-weighted-sum-nonlinearity). A popular one is the rectified linear unit ([torch.nn.ReLU()](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU)).\n",
    "\n",
    "Here, we propose to approximate our sinusoidal data with a simple network made up with 2 linear layers and 1 activation function in between, i.e., a *simple perceptron*:\n",
    "\n",
    "$$x \\to \\textrm{Linear}_1 \\to \\textrm{ReLU} \\to \\textrm{Linear}_2 \\to y$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the 'nonlin_reg_model' model.\n",
    "# - the first linear layer should have input_size=1 and output_size=3 (we increase the dimension of x)\n",
    "# - the second linear layer should have input_size=3 and output_size=1 (we go back to the dimension y)\n",
    "# - to build the complete model, you can stack the separate layers together into one model using torch.nn.Sequential (check the doc!)\n",
    "# - once it's done, compute and print nonlin_reg_model(x) to be sure there are no bugs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialization (ensure reproducibility: everybody should have the same results)\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "torch.manual_seed(0)\n",
    "nonlin_reg_model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.2.3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write the training loop (it is very similar to the linear regression procedure)\n",
    "# - loss function: MSE\n",
    "# - number of epochs: 50\n",
    "# - optimizer: SGD with a learning rate of 0.1\n",
    "# Remember to record the loss over epochs in an array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the loss over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute the predicted outputs, and plot the results (original data and predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. More advanced example: Classification with MNIST\n",
    "## 4.1. Dataset\n",
    "We will use the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, which is a large image dataset of hand-written digits.\n",
    "\n",
    "<center><a href=\"https://en.wikipedia.org/wiki/MNIST_database\">\n",
    "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/2/27/MnistExamples.png\"></a></center>\n",
    "\n",
    "Just like some other widely-used datasets, MNIST can be downloaded directly from Pytorch, and includes specific commands to create a `Dataset` object, a convenient data structure to manipulate datasets in Pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the following libraries to download MNIST, create data and copy models.\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, random_split, Subset\n",
    "import copy\n",
    "\n",
    "#TODO: If this cell triggers an error, install the necessary libraries in your environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data repository\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Choose one (or several) transform(s) to preprocess the data\n",
    "# Here, we transform it to torch tensors, and we normalize the data (to zero-mean and unit-variance)\n",
    "data_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                                  torchvision.transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# Create a Dataset (you can download the data by setting 'download=True')\n",
    "train_data = torchvision.datasets.MNIST(data_dir, train=True, download=True, transform=data_transforms)\n",
    "test_data = torchvision.datasets.MNIST(data_dir, train=False, download=True, transform=data_transforms)\n",
    "\n",
    "num_classes = len(train_data.classes)\n",
    "\n",
    "# We take a subset of 2000 image samples to form the training data:\n",
    "train_data = Subset(train_data, torch.arange(2000))\n",
    "\n",
    "print('Number of classes in the dataset:', num_classes)\n",
    "print('Number of images in the train dataset', len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch one data pair (one image and the corresponding label)\n",
    "image, label = train_data[0]\n",
    "print(image.shape)\n",
    "print('Image label =', label)\n",
    "\n",
    "# You should note that the size of the image is [1, 28, 28], which corresponds to [num_channels, height, width]\n",
    "# Indeed, MNIST images are in black and white so there is only 1 color channel.\n",
    "# To plot this image, we need to remove this channel dimension by using squeeze()\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image.squeeze(), cmap='gray_r')\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Dataloader\n",
    "\n",
    "A `Dataset` object stores all the data, and it might be very large. In practice, when using deep neural networks, we want to divide it into small packs (or *minibatches*) of data, in order to feed the network and compute stochatstic gradient descent. To that end, we create a `Dataloader`: it's a python class which samples over the dataset (that's the *batch sampler*) and assembles the data and labels (using a *collate function*) to generate batches.\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<center><a href=\"https://twitter.com/i/status/1363494433715552259\">\n",
    "    <img src=\"https://pbs.twimg.com/ext_tw_video_thumb/1363493414361305099/pu/img/x_qwSxBU2l0o5Y2z.jpg\" width=\"500\"></a>\n",
    "</center>\n",
    "<center>\n",
    "(click on the image above to check the animation)\n",
    "</center>\n",
    "\n",
    "The good news is that there's a Pytorch function that does it automatically, so we don't need to bother with coding the batch sampler nor the collate function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the batch size (=number of samples/images in each batch) and create the dataloader with random batching\n",
    "batch_size = 8\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "print('Number of batches in the training set: ', len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of images and corresponding labels from the train dataloader\n",
    "batch_example = next(iter(train_dataloader)) # We create an iterator from the dataloader, and get the first item from the iterator\n",
    "image_batch_example = batch_example[0]  # NOTE: - the first element of the obtained torch tensor contains ALL images in the batch\n",
    "labels_batch_example = batch_example[1] #       - the second element contains ALL corresponding labels\n",
    "\n",
    "# Print the size of the batch of images and labels\n",
    "print(image_batch_example.shape)\n",
    "print(labels_batch_example.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images in the batch (along with the corresponding label)\n",
    "plt.figure(figsize = (10,6))\n",
    "for ib in range(batch_size):\n",
    "    plt.subplot(batch_size // 4, 4, ib+1)\n",
    "    plt.imshow(image_batch_example[ib, :].squeeze(), cmap='gray_r')\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    plt.title('Image label = ' + str(labels_batch_example[ib].item()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To feed the images to a multilayer perceptron in vector form, we will vectorize them as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_batch = image_batch_example.reshape(image_batch_example.shape[0], -1)\n",
    "print(vectorized_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3. General Multilayer Perceptron module\n",
    "\n",
    "Now, let's create a general MLP classification network. It's a python class that inherits from the general `nn.Module` object, and it should contain at least 2 methods:\n",
    "\n",
    "- `__init__`, which initializes the network when instanciated (creates all the layers and stores some useful parameters if needed).\n",
    "- `forward`, which applies the forward pass (i.e., compute the output 'out' from the input, using the layers).\n",
    "\n",
    "**Note**: Remember that Python classes usually define and use some variables/data/tensors/dictionary etc. internally (this includes network layers) called *attributes*: they should be defined in the `__init__` method with a specific structure (the name should start by `self.`). This allows us to access these attributes in other methods, or after instantiating an object of your class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassif(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers_hidden):\n",
    "        super(MLPClassif, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Sequential(nn.Linear(input_size, hidden_size), nn.ReLU())\n",
    "        self.num_layers_hidden = num_layers_hidden\n",
    "        self.hidden_layers = nn.ModuleList([nn.Sequential(nn.Linear(hidden_size,hidden_size), nn.ReLU()) for l in range(num_layers_hidden)])\n",
    "        self.output_layer = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = self.input_layer(x)\n",
    "        for l in range(self.num_layers_hidden):\n",
    "            y = self.hidden_layers[l](y)\n",
    "        y = self.output_layer(y)\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What do the different parameters of the initialization function correspond to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instanciate an MLP classifier called 'model' that takes vectorized MNIST images as input, with 3 hidden layers of size of 8 and 10 output classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** to call the `forward` method, we simply use model(x) where x is the input:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(vectorized_batch)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two important remarks can be made:\n",
    "\n",
    "- The output `out` of the model has size `[batch_size, num_classes]` while the true labels `labels_batch_example` defined above has size `[batch_size]`. This is because the output `out` is meant to represent *predicted probabilities* for all classes, while `labels_batch_example` simply contains the true labels as integers.\n",
    "- In classification tasks, we want to output _probabilities_ for all classes. However, nothing ensures that `out` corresponds to probabilities, since it is not normalized and we didn't use any output activation function (values can be negative and not sum up to 1).\n",
    "\n",
    "However, when training a classification network, we generally use the [Cross Entropy](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) loss function, which solves both problems at once. This loss is implemented to handle integer-valued labels. Besides, it automatically applies a [Softmax](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) non-linearity to the predicted outputs, which normalizes them as probabilies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose Cross Entropy as loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Compute the error between the predicted labels 'out' and true labels 'labels_batch_example'\n",
    "loss_batch = loss_fn(out, labels_batch_example)\n",
    "print(loss_batch.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. General Training Loop\n",
    "Below is an example of how a typical training function is defined in Pytorch.\n",
    "\n",
    "**Questions:**\n",
    "- What do the input and output variables of this function correspond to? \n",
    "- Which line of code predicts image labels given a batch of images?\n",
    "- Which line of code is responsible to update the network's parameters?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_mlp_classifier(model, train_dataloader, num_epochs, loss_fn, learning_rate, verbose=True):\n",
    "\n",
    "    # Make a copy of the model (avoid changing the model outside this function)\n",
    "    model_tr = copy.deepcopy(model)\n",
    "    \n",
    "    # Set the model in 'training' mode (ensures all parameters' gradients are computed - it's like setting 'requires_grad=True' for all parameters)\n",
    "    model_tr.train()\n",
    "    \n",
    "    # Define the optimizer\n",
    "    optimizer = torch.optim.SGD(model_tr.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Initialize a list for storing the training loss over epochs\n",
    "    train_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        # Initialize the training loss for the current epoch\n",
    "        tr_loss = 0\n",
    "        \n",
    "        # Iterate over batches using the dataloader\n",
    "        for batch_index, (images, labels) in enumerate(train_dataloader):\n",
    "            \n",
    "            # Prepare the inputs and labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            \n",
    "            # Forward pass\n",
    "            labels_predicted = model_tr(images)\n",
    "            loss = loss_fn(labels_predicted, labels)\n",
    "            \n",
    "            # Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update the current epoch loss\n",
    "            # Note that 'loss.item()' is the loss averaged over the batch, so multiply it with the current batch size to get the total batch loss\n",
    "            tr_loss += loss.item() * images.shape[0]\n",
    "\n",
    "        # At the end of each epoch, get the average training loss and store it\n",
    "        tr_loss = tr_loss/len(train_dataloader.dataset)\n",
    "        train_losses.append(tr_loss)\n",
    "        \n",
    "        # Display the training loss\n",
    "        if verbose:\n",
    "            print('Epoch [{}/{}], Training loss: {:.4f}'.format(epoch+1, num_epochs, tr_loss))\n",
    "    \n",
    "    return model_tr, train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the function defined above to train the classification model on the MNIST training set\n",
    "# Training parameters:\n",
    "# - 30 epochs\n",
    "# - learning rate = 0.01\n",
    "# - loss function: Cross Entropy\n",
    "# Display the loss over epochs using matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5. Evaluation\n",
    "\n",
    "Now that the model is trained, we can evaluate it on the test dataset. We do that by predicting the labels using our model, and comparing it with the true labels. This allows us to compute the classification accuracy, which is provided in the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function: similar to the training loop, except we don't need to compute any gradient / backprop\n",
    "\n",
    "def eval_mlp_classifier(model, eval_dataloader):\n",
    "    \n",
    "    # Set the model in 'evaluation' mode (this disables some layers (batch norm, dropout...) which are not needed when testing)\n",
    "    model.eval() \n",
    "\n",
    "    # In evaluation phase, we don't need to compute gradients (for memory efficiency)\n",
    "    with torch.no_grad():\n",
    "        # initialize the total and correct number of labels to compute the accuracy\n",
    "        correct_labels = 0\n",
    "        total_labels = 0\n",
    "        \n",
    "        # Iterate over the dataset using the dataloader\n",
    "        for images, labels in eval_dataloader:\n",
    "\n",
    "            # Get the predicted labels\n",
    "            images = images.reshape(images.shape[0], -1)\n",
    "            y_predicted = model(images)\n",
    "            \n",
    "            # To get the predicted labels, we need to get the max over all possible classes\n",
    "            _, label_predicted = torch.max(y_predicted.data, 1)\n",
    "            \n",
    "            # Compute accuracy: count the total number of samples, and the correct labels (compare the true and predicted labels)\n",
    "            total_labels += labels.size(0)\n",
    "            correct_labels += (label_predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct_labels / total_labels\n",
    "    \n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# - Create a test dataloader from the test_data database object downloaded above.\n",
    "# - Evaluate the trained model on the test set using the above function and print the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Bonus: use a larger model\n",
    "As a last exercice, train now a larger model with 5 hidden layers of size 10, with the same learning rate and number of epochs. Display the loss over epochs, and print the accuracy of the model over the test set. What do you observe and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summerschool25 (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
