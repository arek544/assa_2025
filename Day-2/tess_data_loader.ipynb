{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e48452c3",
   "metadata": {},
   "source": [
    "# TESS Dataset - Toronto Emotional Speech Set\n",
    "\n",
    "This notebook loads and explores the TESS (Toronto Emotional Speech Set) dataset.\n",
    "\n",
    "The dataset contains audio files organized by emotion categories:\n",
    "- Angry\n",
    "- Disgust\n",
    "- Fear\n",
    "- Happy\n",
    "- Neutral\n",
    "- Pleasant Surprise\n",
    "- Sad\n",
    "\n",
    "Two speakers: OAF (Older Adult Female) and YAF (Younger Adult Female)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe38c017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e76e8127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Dataset not found at ../datasets/kaggle_speech_emotion/TESS Toronto emotional speech set data\n",
      "Please run the download script first.\n"
     ]
    }
   ],
   "source": [
    "# Define dataset path\n",
    "DATASET_PATH = Path('../datasets/kaggle_speech_emotion/TESS Toronto emotional speech set data')\n",
    "\n",
    "# Check if dataset exists\n",
    "if not DATASET_PATH.exists():\n",
    "    print(f\"ERROR: Dataset not found at {DATASET_PATH}\")\n",
    "    print(\"Please run the download script first.\")\n",
    "else:\n",
    "    print(f\"Dataset found at: {DATASET_PATH}\")\n",
    "    print(f\"\\nSubdirectories:\")\n",
    "    for item in sorted(DATASET_PATH.iterdir()):\n",
    "        if item.is_dir():\n",
    "            num_files = len(list(item.glob('*.wav')))\n",
    "            print(f\"  {item.name}: {num_files} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6ac78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all audio file paths and create a dataframe\n",
    "def load_dataset_info(dataset_path):\n",
    "    \"\"\"\n",
    "    Load information about all audio files in the dataset.\n",
    "    Returns a pandas DataFrame with file paths, emotions, and speakers.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for emotion_dir in sorted(dataset_path.iterdir()):\n",
    "        if not emotion_dir.is_dir():\n",
    "            continue\n",
    "            \n",
    "        # Parse directory name to extract speaker and emotion\n",
    "        dir_name = emotion_dir.name\n",
    "        \n",
    "        # Skip nested TESS directory if it exists\n",
    "        if 'TESS Toronto' in dir_name:\n",
    "            continue\n",
    "            \n",
    "        parts = dir_name.split('_')\n",
    "        if len(parts) >= 2:\n",
    "            speaker = parts[0]  # OAF or YAF\n",
    "            emotion = '_'.join(parts[1:])  # emotion name\n",
    "            \n",
    "            # Find all .wav files\n",
    "            for audio_file in emotion_dir.glob('*.wav'):\n",
    "                data.append({\n",
    "                    'file_path': str(audio_file),\n",
    "                    'file_name': audio_file.name,\n",
    "                    'speaker': speaker,\n",
    "                    'emotion': emotion.lower(),\n",
    "                    'emotion_dir': dir_name\n",
    "                })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Load dataset\n",
    "df = load_dataset_info(DATASET_PATH)\n",
    "print(f\"Total audio files: {len(df)}\")\n",
    "print(f\"\\nDataset info:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da761e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset statistics\n",
    "print(\"=== Dataset Statistics ===\")\n",
    "print(f\"\\nTotal samples: {len(df)}\")\n",
    "print(f\"\\nEmotions distribution:\")\n",
    "print(df['emotion'].value_counts().sort_index())\n",
    "print(f\"\\nSpeakers distribution:\")\n",
    "print(df['speaker'].value_counts())\n",
    "print(f\"\\nSamples per speaker and emotion:\")\n",
    "print(pd.crosstab(df['speaker'], df['emotion']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d49de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize emotion distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Emotion distribution\n",
    "emotion_counts = df['emotion'].value_counts().sort_index()\n",
    "axes[0].bar(range(len(emotion_counts)), emotion_counts.values, color='steelblue')\n",
    "axes[0].set_xticks(range(len(emotion_counts)))\n",
    "axes[0].set_xticklabels(emotion_counts.index, rotation=45, ha='right')\n",
    "axes[0].set_xlabel('Emotion')\n",
    "axes[0].set_ylabel('Number of Samples')\n",
    "axes[0].set_title('Emotion Distribution in TESS Dataset')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Speaker and emotion distribution\n",
    "speaker_emotion = pd.crosstab(df['emotion'], df['speaker'])\n",
    "speaker_emotion.plot(kind='bar', ax=axes[1], color=['coral', 'skyblue'])\n",
    "axes[1].set_xlabel('Emotion')\n",
    "axes[1].set_ylabel('Number of Samples')\n",
    "axes[1].set_title('Samples per Speaker and Emotion')\n",
    "axes[1].legend(title='Speaker')\n",
    "axes[1].set_xticklabels(axes[1].get_xticklabels(), rotation=45, ha='right')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34defdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze a sample audio file\n",
    "def analyze_audio(file_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Load an audio file and extract basic information.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Calculate duration\n",
    "    duration = librosa.get_duration(y=y, sr=sr)\n",
    "    \n",
    "    return y, sr, duration\n",
    "\n",
    "# Sample one file from each emotion\n",
    "print(\"Sample audio files from each emotion:\\n\")\n",
    "sample_files = df.groupby('emotion').first()\n",
    "\n",
    "durations = []\n",
    "for emotion, row in sample_files.iterrows():\n",
    "    y, sr, duration = analyze_audio(row['file_path'])\n",
    "    durations.append(duration)\n",
    "    print(f\"{emotion:20s}: {row['file_name']:40s} - {duration:.2f}s\")\n",
    "\n",
    "print(f\"\\nAverage duration: {np.mean(durations):.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f874e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample waveforms and spectrograms for different emotions\n",
    "emotions_to_plot = ['angry', 'happy', 'sad', 'neutral']\n",
    "\n",
    "fig, axes = plt.subplots(len(emotions_to_plot), 2, figsize=(16, 12))\n",
    "\n",
    "for idx, emotion in enumerate(emotions_to_plot):\n",
    "    # Get a sample file for this emotion\n",
    "    sample = df[df['emotion'] == emotion].iloc[0]\n",
    "    y, sr = librosa.load(sample['file_path'], sr=22050)\n",
    "    \n",
    "    # Plot waveform\n",
    "    librosa.display.waveshow(y, sr=sr, ax=axes[idx, 0])\n",
    "    axes[idx, 0].set_title(f'{emotion.capitalize()} - Waveform')\n",
    "    axes[idx, 0].set_xlabel('Time (s)')\n",
    "    axes[idx, 0].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "    img = librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='hz', ax=axes[idx, 1])\n",
    "    axes[idx, 1].set_title(f'{emotion.capitalize()} - Spectrogram')\n",
    "    fig.colorbar(img, ax=axes[idx, 1], format='%+2.0f dB')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nVisualized waveforms and spectrograms for sample emotions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c01853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play sample audio files (one from each emotion)\n",
    "print(\"Play sample audio files:\\n\")\n",
    "\n",
    "for emotion in df['emotion'].unique()[:4]:  # Play first 4 emotions\n",
    "    sample = df[df['emotion'] == emotion].iloc[0]\n",
    "    print(f\"\\n{emotion.capitalize()}: {sample['file_name']}\")\n",
    "    display(Audio(sample['file_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae035bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract audio features for analysis\n",
    "def extract_features(file_path, sr=22050):\n",
    "    \"\"\"\n",
    "    Extract audio features using librosa.\n",
    "    Returns a dictionary of features.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    y, sr = librosa.load(file_path, sr=sr)\n",
    "    \n",
    "    # Extract features\n",
    "    features = {}\n",
    "    \n",
    "    # MFCCs (Mel-frequency cepstral coefficients)\n",
    "    mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    features['mfcc_mean'] = np.mean(mfccs, axis=1)\n",
    "    features['mfcc_std'] = np.std(mfccs, axis=1)\n",
    "    \n",
    "    # Spectral features\n",
    "    features['spectral_centroid'] = np.mean(librosa.feature.spectral_centroid(y=y, sr=sr))\n",
    "    features['spectral_rolloff'] = np.mean(librosa.feature.spectral_rolloff(y=y, sr=sr))\n",
    "    features['zero_crossing_rate'] = np.mean(librosa.feature.zero_crossing_rate(y))\n",
    "    \n",
    "    # Chroma features\n",
    "    chroma = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "    features['chroma_mean'] = np.mean(chroma)\n",
    "    \n",
    "    # RMS energy\n",
    "    features['rms'] = np.mean(librosa.feature.rms(y=y))\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Extract features for a few samples\n",
    "print(\"Extracting features from sample files...\")\n",
    "sample_features = []\n",
    "\n",
    "for emotion in df['emotion'].unique()[:3]:\n",
    "    sample = df[df['emotion'] == emotion].iloc[0]\n",
    "    features = extract_features(sample['file_path'])\n",
    "    features['emotion'] = emotion\n",
    "    sample_features.append(features)\n",
    "\n",
    "# Display features\n",
    "features_df = pd.DataFrame(sample_features)\n",
    "print(\"\\nSample features:\")\n",
    "print(features_df[['emotion', 'spectral_centroid', 'spectral_rolloff', 'zero_crossing_rate', 'rms']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca22d36",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Loading the TESS dataset structure\n",
    "2. ✅ Analyzing dataset distribution by emotion and speaker\n",
    "3. ✅ Visualizing waveforms and spectrograms\n",
    "4. ✅ Playing sample audio files\n",
    "5. ✅ Extracting audio features (MFCCs, spectral features, etc.)\n",
    "\n",
    "**Next steps:**\n",
    "- Extract features from all audio files\n",
    "- Build a machine learning model for emotion classification\n",
    "- Train and evaluate the model\n",
    "- Perform cross-validation and hyperparameter tuning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "assa_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
